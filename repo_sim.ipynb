{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e52f1054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nasun/solexa_sun/lab_members/nasun/tools/miniconda3/envs/haru_repo_envi/lib/python3.11/site-packages/dask/dataframe/__init__.py:31: FutureWarning: The legacy Dask DataFrame implementation is deprecated and will be removed in a future version. Set the configuration option `dataframe.query-planning` to `True` or None to enable the new Dask Dataframe implementation and silence this warning.\n",
      "  warnings.warn(\n",
      "/home/nasun/solexa_sun/lab_members/nasun/tools/miniconda3/envs/haru_repo_envi/lib/python3.11/site-packages/numba/core/decorators.py:246: RuntimeWarning: nopython is set for njit and is ignored\n",
      "  warnings.warn('nopython is set for njit and is ignored', RuntimeWarning)\n",
      "/home/nasun/solexa_sun/lab_members/nasun/tools/miniconda3/envs/haru_repo_envi/lib/python3.11/site-packages/anndata/utils.py:429: FutureWarning: Importing read_text from `anndata` is deprecated. Import anndata.io.read_text instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "WARNING:2025-11-29 23:22:56,066:jax._src.xla_bridge:966: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('Haruka')\n",
    "\n",
    "from Haruka import Haruka \n",
    "\n",
    "\n",
    "import scvi\n",
    "\n",
    "scvi.utils.seed = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38bd6924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc \n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score as ari\n",
    "\n",
    "from sklearn.metrics import normalized_mutual_info_score as nmi\n",
    "\n",
    "\n",
    "def run_Haruka(adata):\n",
    "\n",
    "    scvi.utils.seed = 33\n",
    "    model = Haruka(adata, cov_gene_num=64)\n",
    "\n",
    "    model.construct_cov()\n",
    "\n",
    "    model.setup_model(gene_likelihood='nb', condition_label='condition', wasserstein_penalty=0.4, me_weight=155/(64*64))\n",
    "\n",
    "    model.train(max_epochs=200)\n",
    "\n",
    "    model.extract_rep(n_layer=2)\n",
    "\n",
    "    model.cluster(background_cluster_number=8, salient_cluster_num=3, seed=33)\n",
    "\n",
    "    res_haruka = model.output_adata\n",
    "\n",
    "    return res_haruka \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a3998c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import *\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "def _compute_CHAOS(clusterlabel,location):\n",
    "\n",
    "    clusterlabel = np.array(clusterlabel)\n",
    "    location = np.array(location)\n",
    "    matched_location = StandardScaler().fit_transform(location)\n",
    "#     matched_location = location\n",
    "    clusterlabel_unique = np.unique(clusterlabel)\n",
    "    dist_val = np.zeros(len(clusterlabel_unique))\n",
    "    count = 0\n",
    "    for k in clusterlabel_unique:\n",
    "        location_cluster = matched_location[clusterlabel==k,:]\n",
    "        if len(location_cluster)<=2:\n",
    "            continue\n",
    "        n_location_cluster = len(location_cluster)\n",
    "        results = [fx_1NN(i,location_cluster) for i in range(n_location_cluster)]\n",
    "        dist_val[count] = np.sum(results)\n",
    "        count = count + 1\n",
    "\n",
    "\n",
    "    return np.sum(dist_val)/len(clusterlabel)\n",
    "\n",
    "def fx_1NN(i,location_in):\n",
    "    location_in = np.array(location_in)\n",
    "    dist_array = distance_matrix(location_in[i,:][None,:],location_in)[0,:]\n",
    "    dist_array[i] = np.inf\n",
    "    return np.min(dist_array)\n",
    "    \n",
    "def _compute_PAS(clusterlabel,location):\n",
    "    \n",
    "    clusterlabel = np.array(clusterlabel)\n",
    "    location = np.array(location)\n",
    "    matched_location = location\n",
    "    results = [fx_kNN(i,matched_location,k=10,cluster_in=clusterlabel) for i in range(matched_location.shape[0])]\n",
    "    return np.sum(results)/len(clusterlabel)\n",
    "\n",
    "\n",
    "def fx_kNN(i,location_in,k,cluster_in):\n",
    "#     print(i)\n",
    "\n",
    "# def\n",
    "    location_in = np.array(location_in)\n",
    "    cluster_in = np.array(cluster_in)\n",
    "\n",
    "\n",
    "    dist_array = distance_matrix(location_in[i,:][None,:],location_in)[0,:]\n",
    "    dist_array[i] = np.inf\n",
    "    ind = np.argsort(dist_array)[:k]\n",
    "    cluster_use = np.array(cluster_in)\n",
    "    if np.sum(cluster_use[ind]!=cluster_in[i])>(k/2):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbcc2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import squidpy as sq\n",
    "\n",
    "ari_res_s = []\n",
    "\n",
    "pas_res_s = []\n",
    "\n",
    "chaos_res_s = []\n",
    "\n",
    "nmi_res_s = []\n",
    "\n",
    "ari_res_b = []\n",
    "\n",
    "pas_res_b = []\n",
    "\n",
    "nmi_res_b = []\n",
    "\n",
    "chaos_res_b = []\n",
    "\n",
    "for i, path in enumerate(['sim_bench.h5ad', 'sim_bench_s.h5ad', 'sim_bench_b.h5ad']):\n",
    "\n",
    "    #adata = sc\n",
    "\n",
    "    adata = sc.read_h5ad('/lab/solexa_sun/lab_members/yancui/' + path)\n",
    "\n",
    "    label_encoder = {\"circle\":1, 'square':1, 'other':0, 'triangle':2}\n",
    "\n",
    "    adata.obs['label'] = adata.obs['shape_region'].map(label_encoder).astype(str)\n",
    "\n",
    "    adata = run_Haruka(adata)\n",
    "\n",
    "    \n",
    "    sq.pl.spatial_scatter(adata, color=['cluster_haruka_salient', 'cluster_haruka_background', 'label', 'Region'], shape=None)\n",
    "\n",
    "    ari_res_s.append(ari(adata.obs['cluster_haruka_salient'].values, adata.obs['label'].values))\n",
    "\n",
    "    nmi_res_s.append(nmi(adata.obs['cluster_haruka_salient'].values, adata.obs['label'].values))\n",
    "\n",
    "    ari_res_b.append(ari(adata.obs['cluster_haruka_background'].values, adata.obs['Region'].values))\n",
    "\n",
    "    nmi_res_b.append(nmi(adata.obs['cluster_haruka_background'].values, adata.obs['Region'].values))\n",
    "\n",
    "    pas_res_s.append(_compute_PAS(adata.obs['cluster_haruka_salient'].values, adata.obsm['spatial']))\n",
    "\n",
    "    pas_res_b.append(_compute_PAS(adata.obs['cluster_haruka_background'].values, adata.obsm['spatial']))\n",
    "\n",
    "    chaos_res_s.append(_compute_CHAOS(adata.obs['cluster_haruka_salient'].values, adata.obsm['spatial']))\n",
    "    \n",
    "    chaos_res_b.append(_compute_CHAOS(adata.obs['cluster_haruka_background'].values, adata.obsm['spatial']))\n",
    "\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
